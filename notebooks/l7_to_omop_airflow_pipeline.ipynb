{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f690591",
   "metadata": {},
   "source": [
    "# 📊 Data Ingestion Pipeline: L7 to OMOP CDM via Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd875e87",
   "metadata": {},
   "source": [
    "\n",
    "## 🔍 What is Apache Airflow?\n",
    "\n",
    "Apache Airflow is an open-source platform to programmatically author, schedule, and monitor workflows. Think of it as a workflow orchestrator for your data pipelines.\n",
    "\n",
    "### Benefits:\n",
    "- **Modular**: Each task is defined in Python.\n",
    "- **Scalable**: Handles complex workflows with retries and dependencies.\n",
    "- **Visual**: Provides a UI to track job execution and dependencies.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58e1aa2",
   "metadata": {},
   "source": [
    "\n",
    "## 🏗️ Use Case: Ingesting Clinical Data from L7 (Postgres)\n",
    "\n",
    "The objective is to extract patient data from the L7 Postgres database, apply lightweight transformation to conform with OMOP CDM, and load it into the OMOP-compliant database.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da905d93",
   "metadata": {},
   "source": [
    "\n",
    "## 🧠 How the DAG Works\n",
    "\n",
    "- **Start** ➡️ **Extract from L7** ➡️ **Transform Data** ➡️ **Load to OMOP DB** ➡️ **End**\n",
    "\n",
    "Each task in Airflow corresponds to a Python function. The DAG ensures they run in the correct order.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e439eca",
   "metadata": {},
   "source": [
    "\n",
    "## 🧾 Airflow DAG Code Example\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41b356e",
   "metadata": {},
   "source": [
    "\n",
    "## 🛠️ Requirements\n",
    "\n",
    "- Airflow (`pip install apache-airflow`)\n",
    "- PostgreSQL Driver: `psycopg2`, `sqlalchemy`\n",
    "- Airflow running with DAG folder configured (`~/airflow/dags/`)\n",
    "\n",
    "## 📚 Resources\n",
    "\n",
    "- [Airflow Docs](https://airflow.apache.org/docs/apache-airflow/stable/)\n",
    "- [OMOP CDM Info](https://ohdsi.github.io/CommonDataModel/)\n",
    "- [Astronomer Academy](https://www.astronomer.io/learn/)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bd8700",
   "metadata": {},
   "source": [
    "## Code + Explanation:\n",
    "\n",
    "- Imports Airflow modules and data libraries.\n",
    "\n",
    "- ✅ You’ll need psycopg2 and sqlalchemy installed (requirements.txt).\n",
    "\n",
    "- ✅ These will be used to connect to both L7 and OMOP databases.\n",
    "\n",
    "then: \n",
    "\n",
    "- Sets default behavior: 1 retry if a task fails.\n",
    "\n",
    "and:\n",
    "\n",
    "- Defines the DAG object.\n",
    "\n",
    "    - Runs daily. catchup=False means it won’t backfill for past dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6690bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'harbinger',\n",
    "    'start_date': days_ago(1),\n",
    "    'retries': 1\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'l7_to_omop_ingestion',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    "    description='Ingest L7 clinical data from Postgres to OMOP CDM staging area'\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2b6296",
   "metadata": {},
   "source": [
    "## 💡 Required from Harbinger for this section:\n",
    "\n",
    "| **Need**                                                            | **Why**                                               |\n",
    "| ------------------------------------------------------------------- | ----------------------------------------------------- |\n",
    "| ✅ L7 PostgreSQL host, port, database name, user, and password       | To extract source clinical data                       |\n",
    "| ✅ Table names and sample schema                                     | So you can define `SELECT` queries properly           |\n",
    "| ✅ OMOP Postgres DB connection (or Redshift/Snowflake, if different) | To load transformed data                              |\n",
    "| ✅ Mapping from L7 schema → OMOP CDM schema                          | For transforming fields properly                      |\n",
    "| ✅ Data dictionary if available                                      | To help with transformation logic and standardization |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e4cd43",
   "metadata": {},
   "source": [
    "### Step1: L7 Data Extraction from Postgres\n",
    "\n",
    "- Connects to the L7 Postgres database.\n",
    "\n",
    "- Pulls raw patient data.\n",
    "\n",
    "- Saves it to disk for transformation.\n",
    "\n",
    "✅ What is NEEDED:\n",
    "\n",
    "- Actual patient table names\n",
    "\n",
    "- Column structure\n",
    "\n",
    "- Filtering logic (e.g., only recent patients?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_data_from_l7():\n",
    "    conn = psycopg2.connect(\n",
    "        host='your-l7-host',\n",
    "        dbname='l7_database',\n",
    "        user='your_user',\n",
    "        password='your_password',\n",
    "        port=5432\n",
    "    )\n",
    "    df = pd.read_sql(\"SELECT * FROM patient_table\", conn)\n",
    "    df.to_csv('/tmp/l7_patient_data.csv', index=False)\n",
    "    conn.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b4001",
   "metadata": {},
   "source": [
    "### Step 2: Transform to OMOP format\n",
    "\n",
    "- Simple transform example: lowercase column names.\n",
    "\n",
    "- We can replace this with logic that maps to OMOP CDM specs.\n",
    "\n",
    "✅ Needed:\n",
    "\n",
    "- Mapping sheet: L7 columns to OMOP columns\n",
    "\n",
    "- Any standard terminologies (LOINC, SNOMED, etc.) needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d4723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_data():\n",
    "    df = pd.read_csv('/tmp/l7_patient_data.csv')\n",
    "    df.columns = [col.lower() for col in df.columns]  # sample transformation\n",
    "    df.to_csv('/tmp/transformed_patient_data.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09431be6",
   "metadata": {},
   "source": [
    "### Step 3: Load to OMOP\n",
    "\n",
    "- Loads to the person table in the OMOP schema.\n",
    "\n",
    "✅ Needed:\n",
    "\n",
    "- The target schema and table (e.g., staging or production)\n",
    "\n",
    "- Whether deduplication or upserts are required\n",
    "\n",
    "- Which layer this sits in (Bronze, Silver, Gold?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f351980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_to_omop():\n",
    "    engine = sqlalchemy.create_engine('postgresql://omop_user:password@omop-host:5432/omop_db')\n",
    "    df = pd.read_csv('/tmp/transformed_patient_data.csv')\n",
    "    df.to_sql('person', engine, if_exists='append', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce17dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extract_task = PythonOperator(\n",
    "    task_id='extract_l7',\n",
    "    python_callable=extract_data_from_l7,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "transform_task = PythonOperator(\n",
    "    task_id='transform_l7',\n",
    "    python_callable=transform_data,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "load_task = PythonOperator(\n",
    "    task_id='load_omop',\n",
    "    python_callable=load_to_omop,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "extract_task >> transform_task >> load_task\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a9439",
   "metadata": {},
   "source": [
    "## Summary of What is Needed from Harbinger \n",
    "\n",
    "| **Input Needed**                         | **Why**                                      |\n",
    "| ---------------------------------------- | -------------------------------------------- |\n",
    "| ✅ L7 connection details                  | To extract raw data                          |\n",
    "| ✅ Table/schema information               | What and how much data to extract            |\n",
    "| ✅ Transformation logic                   | L7 → OMOP mapping                            |\n",
    "| ✅ OMOP target DB + credentials           | Where to load the data                       |\n",
    "| ✅ Data refresh frequency                 | How often DAG should run (daily/weekly)      |\n",
    "| ✅ Data dictionary (if available)         | Ensures field alignment and semantic clarity |\n",
    "| ✅ Any PII/PHI concerns or masking needed | For compliance with HIPAA                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586445f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
